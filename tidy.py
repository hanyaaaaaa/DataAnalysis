import pandas as pd
from bs4 import BeautifulSoup
import re
import os
import requests
import pdfplumber

# è¨­å®šè³‡æ–™å¤¾å’Œè¼¸å‡ºè·¯å¾‘
input_dir = "C:/Users/æ/Desktop/æ•¸æ“šåˆ†æ"
output_path = "C:/Users/æ/Desktop/æ•¸æ“šåˆ†æ/importantcsv/import.csv"

# ä¸­æ–‡æ•¸å­—è½‰æ›
def chinese_to_number(text):
    mapping = {'é›¶': 0, 'å£¹': 1, 'è²³': 2, 'åƒ': 3, 'è‚†': 4, 'ä¼': 5, 'é™¸': 6, 'æŸ’': 7, 'æŒ': 8, 'ç–': 9, 
               'æ‹¾': 10, 'ä½°': 100, 'ä»Ÿ': 1000, 'è¬': 10000}
    num = 0
    temp = 0
    for char in text:
        if char in mapping:
            if char in 'æ‹¾ä½°ä»Ÿè¬':
                temp = temp or 1
                num += temp * mapping[char]
                temp = 0
            else:
                temp = mapping[char]
    return num + temp if temp else num

# åˆå§‹åŒ–è³‡æ–™åˆ—è¡¨
all_cases = []

# è¿´åœˆè™•ç†æª”æ¡ˆ
for i in range(1, 411):  
    file_path = os.path.join(input_dir, f"case_{i}_detail.html")
    
    if not os.path.exists(file_path):
        print(f"âš ï¸ æª”æ¡ˆä¸å­˜åœ¨ï¼š{file_path}")
        continue

    # è®€å– HTML
    with open(file_path, "r", encoding="utf-8") as file:
        html_content = file.read()

    soup = BeautifulSoup(html_content, "html.parser")

    # å¾ <title> æå–åŸºæœ¬è³‡è¨Š
    court_name = soup.find("title").text.strip().split(" ")[0] if soup.find("title") else "æœªçŸ¥"
    title_text = soup.find("title").text.strip() if soup.find("title") else ""
    # å¾ <title> æå–æ—¥æœŸ
    date_match = re.search(r"(\d{3,4})\s*å¹´åº¦", title_text)
    judgment_date = f"æ°‘åœ‹ {date_match.group(1)} å¹´" if date_match else "æœªçŸ¥"
    # å¾ HTML æå–æ›´ç²¾ç¢ºæ—¥æœŸ
    date_elem = soup.select_one(".int-table .row:nth-child(2) .col-td")
    if date_elem:
        judgment_date = date_elem.text.strip()
    case_type = "è²è«‹å®šæ‡‰åŸ·è¡Œåˆ‘" if "è²å­—ç¬¬" in title_text else "æœªçŸ¥"

    # æå–ç›¸é—œæ³•æ¢ã€ç½ªåã€åˆ‘æœŸå’Œä¸Šä¸‹æ–‡
    laws = []
    offenses = []
    sentences = []
    raw_content = []

    # å¾ PDF æå–
    pdf_link = soup.find("a", id="hlExportPDF")
    if pdf_link:
        pdf_url = "https://judgment.judicial.gov.tw" + pdf_link["href"]
        pdf_path = os.path.join(input_dir, f"case_{i}_detail.pdf")
        
        try:
            response = requests.get(pdf_url, timeout=10)
            with open(pdf_path, "wb") as pdf_file:
                pdf_file.write(response.content)

            with pdfplumber.open(pdf_path) as pdf:
                text = "".join(page.extract_text() or "" for page in pdf.pages if page.extract_text())
                
                # æå–æ—¥æœŸï¼ˆå‚™ç”¨ï¼‰
                if judgment_date == "æœªçŸ¥":
                    date_match = re.search(r"æ°‘åœ‹\s*(\d{3,4})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥", text)
                    if date_match:
                        judgment_date = f"æ°‘åœ‹ {date_match.group(1)} å¹´ {date_match.group(2)} æœˆ {date_match.group(3)} æ—¥"
                
                # æå–æ¡ˆä»¶é¡å‹
                if case_type == "æœªçŸ¥":
                    case_match = re.search(r"è£åˆ¤æ¡ˆç”±\D*([^\n]+)", text)
                    case_type = case_match.group(1).strip() if case_match else "æœªçŸ¥"
                
                # æå–ä¸»æ–‡
                main_match = re.search(r"ä¸»\s*æ–‡\s*(.+?)(?:äº‹\s*å¯¦|ç†\s*ç”±|$)", text, re.DOTALL)
                if main_match:
                    main_content = main_match.group(1).strip()
                    raw_content.append(re.sub(r'\n\d+\s*', ' ', main_content))  # ç§»é™¤è¡Œè™Ÿ
                    offense_match = re.findall(r"çŠ¯(.+?ç½ª)", main_content)
                    sentence_match = re.findall(r"è™•\s*([^ï¼Œã€‚]+[å¹´æœˆæ—¥])|æ‡‰åŸ·è¡Œ([^ï¼Œã€‚]+[å¹´æœˆæ—¥])", main_content)
                    if offense_match:
                        offenses.extend([o.strip() for o in offense_match])
                    if sentence_match:
                        for match in sentence_match:
                            sentence = next((s for s in match if s), None)
                            if sentence:
                                sentences.append(sentence.strip())
                
                # æå–é™„è¡¨
                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        if table and len(table) > 1:
                            headers = table[0]
                            offense_idx = -1
                            sentence_idx = -1
                            for idx, header in enumerate(headers):
                                if header and "ç½ª" in header and "æ—¥æœŸ" not in header:
                                    offense_idx = idx
                                if header and "åˆ‘" in header:
                                    sentence_idx = idx
                            if offense_idx != -1 and sentence_idx != -1:
                                for row in table[1:]:
                                    if len(row) > max(offense_idx, sentence_idx):
                                        offense = row[offense_idx].strip() if row[offense_idx] else ""
                                        sentence = row[sentence_idx].strip() if row[sentence_idx] else "æœªçŸ¥"
                                        if offense and "ç½ª" in offense:
                                            offenses.append(offense)
                                            sentences.append(sentence)
                                            raw_content.append(f"{offense}ï¼Œè™•{sentence}")
                
                # æå–æ³•æ¢
                law_match = re.findall(r"(?:ä¸­è¯æ°‘åœ‹åˆ‘æ³•|åˆ‘äº‹è¨´è¨Ÿæ³•|æ´—éŒ¢é˜²åˆ¶æ³•)[^\n]+", text)
                if law_match:
                    laws.extend(law_match)

            os.remove(pdf_path)
        except Exception as e:
            print(f"âš ï¸ PDF ä¸‹è¼‰æˆ–è§£æå¤±æ•—ï¼š{file_path}ï¼ŒéŒ¯èª¤ï¼š{e}")

    # æ¸…ç†æ ¼å¼
    offenses = list(set(o.strip() for o in offenses if o.strip() and "ç½ª" in o and "æ—¥æœŸ" not in o and "ç·¨è™Ÿ" not in o)) or ["æœªçŸ¥"]
    sentences = [re.sub(r"å¦‚æ˜“ç§‘ç½°é‡‘.*$", "", s.strip()) for s in sentences if s.strip()]
    sentences = [f"{chinese_to_number(re.sub(r'[å¹´æœˆæ—¥]', '', s))}{next((c for c in 'å¹´æœˆæ—¥' if c in s), 'æœˆ' if 'æœ‰æœŸå¾’åˆ‘' in s else 'æ—¥' if 'æ‹˜å½¹' in s else '')}" 
                 for s in sentences if any(c in s for c in "å¹´æœˆæ—¥") or "æœ‰æœŸå¾’åˆ‘" in s or "æ‹˜å½¹" in s] or ["æœªçŸ¥"]

    # è‹¥ç½ªåå«ã€Œå¦‚é™„è¡¨æ‰€ç¤ºã€ï¼Œç”¨é™„è¡¨ç½ªåæ›¿æ›
    if any("é™„è¡¨" in o or "é™„ä»¶" in o for o in offenses):
        offenses = [o for o in offenses if "é™„è¡¨" not in o and "é™„ä»¶" not in o] or ["æœªçŸ¥"]

    # å»ºç«‹æ¡ˆä»¶è³‡æ–™
    case_data = {
        "æ³•é™¢åç¨±": court_name,
        "è£åˆ¤æ—¥æœŸ": judgment_date,
        "æ¡ˆä»¶é¡å‹": case_type,
        "ç½ªå": "; ".join(offenses),
        "åˆ‘æœŸ": "; ".join(sentences),
        "ç›¸é—œæ³•æ¢": "; ".join(laws) if laws else "ç„¡",
        "åŸå§‹å…§å®¹": "; ".join(raw_content) if raw_content else "ç„¡"
    }

    all_cases.append(case_data)
    print(f"âœ… å·²è™•ç†ï¼š{file_path}")

# è½‰æ›ç‚º DataFrame ä¸¦å­˜æˆ CSV
df = pd.DataFrame(all_cases)
df.to_csv(output_path, index=False, encoding="utf-8-sig")
print(f"ğŸ“‚ æ‰€æœ‰æ¡ˆä»¶è³‡æ–™å·²å„²å­˜ç‚ºï¼š{output_path}")