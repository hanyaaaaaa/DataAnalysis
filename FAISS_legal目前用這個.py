import faiss
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# è®€å–æ•¸æ“š
try:
    df = pd.read_csv('C:/Users/æ/Desktop/æ•¸æ“šåˆ†æ/outputarea/Tovector.csv')
except Exception as e:
    print(f"âŒ ç„¡æ³•è®€å–æ•¸æ“šæ–‡ä»¶: {e}")
    exit()

# éæ¿¾ç„¡åˆ‘æœŸæ•¸æ“š
df_with_sentence = df[df['åˆ‘æœŸ(å¤©)'] > 0].copy()
print(f"éæ¿¾å¾Œæ•¸æ“šé‡: {len(df_with_sentence)} è¡Œ")

# è¼‰å…¥åŸå§‹åµŒå…¥æ¨¡å‹
try:
    model = SentenceTransformer('DMetaSoul/sbert-chinese-general-v2')
    print("âœ… å·²è¼‰å…¥é€šç”¨æ¨¡å‹ï¼šDMetaSoul/sbert-chinese-general-v2")
except Exception as e:
    print(f"âŒ ç„¡æ³•è¼‰å…¥æ¨¡å‹: {e}")
    exit()

# è™•ç†æ–‡æœ¬å‘é‡
try:
    text_embeddings = model.encode(df_with_sentence["åŸå§‹å…§å®¹"].tolist()).astype('float32')
except Exception as e:
    print(f"âŒ æ–‡æœ¬å‘é‡åŒ–å¤±æ•—: {e}")
    exit()

# å°‡æ•¸æ“šé›†åˆ†å‰²ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†
train_df, test_df = train_test_split(df_with_sentence, test_size=0.2, random_state=42)
train_embeddings = model.encode(train_df["åŸå§‹å…§å®¹"].tolist()).astype('float32')
test_embeddings = model.encode(test_df["åŸå§‹å…§å®¹"].tolist()).astype('float32')

# è¨­ç½® K-Means åƒæ•¸
ncentroids = max(2, len(train_df) // 20)  # å¢åŠ èšé¡æ•¸é‡åˆ°ç´„ 11
niter = 50       # å¢åŠ è¿­ä»£æ¬¡æ•¸
verbose = True   # é¡¯ç¤ºè¨“ç·´éç¨‹
print(f"èšé¡æ•¸é‡ (ncentroids): {ncentroids}")

# ç²å–åµŒå…¥ç¶­åº¦
d = train_embeddings.shape[1]

# å‰µå»ºé‡åŒ–å™¨ (quantizer)
quantizer = faiss.IndexFlatL2(d)

# å‰µå»ºä½¿ç”¨ K-Means èšé¡çš„ IndexIVFFlat ç´¢å¼•
index = faiss.IndexIVFFlat(quantizer, d, ncentroids, faiss.METRIC_L2)

# è¨“ç·´ K-Means æ¨¡å‹ç”Ÿæˆèšé¡è³ªå¿ƒ
assert not index.is_trained
index.train(train_embeddings)
assert index.is_trained

# å°‡è¨“ç·´æ•¸æ“šæ·»åŠ åˆ°ç´¢å¼•
index.add(train_embeddings)

# ä¿å­˜ç´¢å¼•
faiss.write_index(index, "court_index_ivf_optimized.faiss")

# åˆ‘æœŸé æ¸¬ç³»çµ±
def legal_consult_system(index, train_df, model):
    print("\n== æ™ºèƒ½åˆ‘æœŸè©•ä¼°ç³»çµ±ï¼ˆä½¿ç”¨ K-Means èšé¡èˆ‡å„ªåŒ–åƒæ•¸ï¼‰==")
    while True:
        text = input("\næè¿°æ¡ˆæƒ…ï¼ˆè¼¸å…¥exité€€å‡ºï¼‰: ")
        if text.lower() == 'exit':
            break
        
        try:
            text_vec = model.encode([text]).astype('float32')
            index.nprobe = 75  # å¢åŠ  nprobe åˆ° 75
            k = 7  # èª¿æ•´ k åˆ° 7
            distances, indices = index.search(text_vec, k=k)
            
            print("\nâ˜… ç›¸ä¼¼åˆ¤ä¾‹åˆ†æ:")
            weights = np.exp(-distances[0])  # ä½¿ç”¨æŒ‡æ•¸è¡°æ¸›æ¬Šé‡
            weights /= weights.sum()  # æ­¸ä¸€åŒ–æ¬Šé‡
            for i, idx in enumerate(indices[0]):
                case = train_df.iloc[idx]
                similarity = 1 / (1 + distances[0][i])
                print(f"\nãƒ»{case['åŸå§‹å…§å®¹'][:50]}...")
                print(f"  å¯¦éš›åˆ‘æœŸï¼š{case['åˆ‘æœŸ(å¤©)']}å¤© | ç›¸ä¼¼åº¦ï¼š{similarity:.2%}")
            
            similar_sentences = train_df.iloc[indices[0]]['åˆ‘æœŸ(å¤©)']
            weighted_average_sentence = np.average(similar_sentences, weights=weights)
            print(f"\nåŸºæ–¼ç›¸ä¼¼åˆ¤ä¾‹ï¼ŒåŠ æ¬Šé ä¼°åˆ‘æœŸç‚º: {weighted_average_sentence:.2f} å¤©")
        except Exception as e:
            print(f"âŒ è¼¸å…¥éŒ¯èª¤: {e}ï¼Œè«‹é‡æ–°è¼¸å…¥")

# è¨ˆç®—å¤šç¨®è©•ä¼°æŒ‡æ¨™
def calculate_metrics(index, test_df, train_df, model):
    predictions = []
    actuals = []
    for idx, row in test_df.iterrows():
        text_vec = model.encode([row["åŸå§‹å…§å®¹"]]).astype('float32')
        distances, indices = index.search(text_vec, k=7)  # k=7
        weights = np.exp(-distances[0])  # ä½¿ç”¨æŒ‡æ•¸è¡°æ¸›æ¬Šé‡
        weights /= weights.sum()
        similar_sentences = train_df.iloc[indices[0]]['åˆ‘æœŸ(å¤©)']
        predicted_sentence = np.average(similar_sentences, weights=weights)
        predictions.append(predicted_sentence)
        actuals.append(row['åˆ‘æœŸ(å¤©)'])
    
    mse = mean_squared_error(actuals, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(actuals, predictions)
    r2 = r2_score(actuals, predictions)
    
    print(f"\n=== æ¨¡å‹è©•ä¼°æŒ‡æ¨™ï¼ˆå„ªåŒ–å¾Œï¼‰===")
    print(f"MSEï¼ˆå‡æ–¹èª¤å·®ï¼‰: {mse:.2f} å¤©Â²")
    print(f"RMSEï¼ˆå‡æ–¹æ ¹èª¤å·®ï¼‰: {rmse:.2f} å¤©")
    print(f"MAEï¼ˆå¹³å‡çµ•å°èª¤å·®ï¼‰: {mae:.2f} å¤©")
    print(f"RÂ² åˆ†æ•¸ï¼ˆæ±ºå®šä¿‚æ•¸ï¼‰: {r2:.4f}")
    if r2 < 0:
        print("âš ï¸ RÂ² ç‚ºè² æ•¸ï¼Œæ¨¡å‹é æ¸¬æ•ˆæœæ¯”ç°¡å–®å‡å€¼é æ¸¬é‚„å·®ï¼")
    else:
        print(f"ğŸ‰ RÂ² ç‚ºæ­£æ•¸ï¼Œæ¨¡å‹å·²èƒ½æ•æ‰ {r2*100:.1f}% çš„åˆ‘æœŸè®Šç•°æ€§ï¼")

# è¼‰å…¥ç´¢å¼•
try:
    index = faiss.read_index("court_index_ivf_optimized.faiss")
except Exception as e:
    print(f"âŒ è«‹å…ˆå‰µå»ºç´¢å¼•ï¼ŒéŒ¯èª¤: {e}")
    exit()

# å•Ÿå‹•ç³»çµ±
legal_consult_system(index, train_df, model)

# è¨ˆç®—ä¸¦é¡¯ç¤ºè©•ä¼°æŒ‡æ¨™
calculate_metrics(index, test_df, train_df, model)